# ğŸ¤– RANSOMEYE â€“ PHASE 7

## AI / ML Training, Incremental Learning & SHAP Explainability

### **(PROMPT NUMBER 7 OF 12)**

**MASTER PROMPT â€“ CURSOR EXECUTION**

---

## CONTEXT (STRICT BOUNDARIES)

You are now building the **AI / ML layer** of RansomEye.

This layer **assists humans and downstream logic**, but **must never have enforcement authority**.
It must be **fully auditable, reproducible, explainable, and replaceable**.

This phase exists to:

* Improve signal quality
* Reduce false positives
* Provide explainability
* Support SOC analysts

If this phase violates boundaries:

* Trust model breaks
* Legal defensibility collapses
* Determinism is lost

---

## ğŸš¨ ABSOLUTE LANGUAGE RULES

### âœ… ALLOWED LANGUAGES

* **Python (MANDATORY for AI/ML)**
* **Rust / C++ (ONLY as performance extensions via FFI)**

### âŒ FORBIDDEN LANGUAGES

* Java
* Go
* NodeJS
* Any JVM-based runtime
* Any service embedding Python in hot path

â¡ï¸ Python is **ISOLATED** to AI layer only.

---

## ğŸš« LICENSE RULES (STRICT)

Allowed only:

* MIT
* BSD
* Apache 2.0
* PSF

BANNED:

* GPL / AGPL / SSPL
* Unknown licenses
* Copyleft datasets

All datasets must be **synthetic or customer-provided at runtime**.

---

## ğŸ¯ PHASE 7 OBJECTIVE

Build a **fully trainable, incrementally updatable, explainable AI/ML system** that:

1. Trains models on labeled telemetry
2. Supports incremental / continual learning
3. Produces **SHAP explanations for every numeric inference**
4. Emits confidence scores and rationale
5. Operates offline and air-gapped
6. Never blocks or enforces actions directly

---

## ğŸ“¦ AI/ML RESPONSIBILITIES (STRICT)

### MUST DO

* Feature extraction
* Model training
* Incremental updates
* Inference (read-only)
* Explainability artifact generation
* Model versioning & metadata
* Reproducible pipelines

### MUST NEVER DO

* Enforce policy
* Trigger response actions
* Bypass Core decisions
* Train on committed datasets
* Hide or suppress explanations

---

## ğŸ“ PHASE 7 FOLDER STRUCTURE (MANDATORY)

You MUST create and explain:

```
ai/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â”œâ”€â”€ train_model.py
â”‚   â”‚   â”œâ”€â”€ incremental_update.py
â”‚   â”‚   â””â”€â”€ validation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ metadata_schema.json
â”‚   â”‚   â”œâ”€â”€ versioning.py
â”‚   â”‚   â””â”€â”€ registry.py
â”‚   â”‚
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ generators.py   # synthetic only
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ training_config.py
â”‚
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ predictor.py
â”‚   â”œâ”€â”€ confidence.py
â”‚   â””â”€â”€ safeguards.py
â”‚
â”œâ”€â”€ explainability/
â”‚   â”œâ”€â”€ shap_generator.py
â”‚   â”œâ”€â”€ shap_schema.json
â”‚   â””â”€â”€ artifact_writer.py
â”‚
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ core_adapter.py
â”‚   â””â”€â”€ read_only_contract.py
â”‚
â”œâ”€â”€ security/
â”‚   â”œâ”€â”€ model_signing.py
â”‚   â”œâ”€â”€ verification.py
â”‚   â””â”€â”€ trust_validation.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ model_types.md
â”‚   â”œâ”€â”€ training_flow.md
â”‚   â”œâ”€â”€ incremental_learning.md
â”‚   â”œâ”€â”€ explainability.md
â”‚   â””â”€â”€ failure_modes.md
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ synthetic_training_tests.py
    â”œâ”€â”€ explainability_tests.py
    â”œâ”€â”€ drift_tests.py
    â””â”€â”€ integrity_tests.py
```

---

## ğŸ§  MODEL REQUIREMENTS (MANDATORY)

You MUST define:

* Supported model types (tree, linear, ensemble, etc.)
* Feature constraints
* Labeling strategy
* Drift detection
* Retraining triggers

All models MUST:

* Be trainable
* Support incremental updates
* Emit confidence scores
* Emit SHAP explanations

---

## ğŸ” SHAP EXPLAINABILITY (NON-NEGOTIABLE)

You MUST ensure:

* SHAP generated for **every numeric inference**
* Stored alongside prediction
* Schema-defined
* Verifiable and signed
* Human-readable and machine-readable

No â€œbest effortâ€ explanations allowed.

---

## ğŸ” MODEL SECURITY & TRUST

You MUST implement:

* Model signing
* Model verification before load
* Version pinning
* Rollback capability
* Tamper detection

Model verification failure = **BLOCK inference**

---

## ğŸ”€ AI â†” CORE INTERACTION (STRICT)

Define:

* Read-only interface
* Explicit data contract
* One-way flow (Core â†’ AI â†’ Human)
* No feedback loops without validation

AI output:

* Scores
* Explanations
* Recommendations

Never commands.

---

## ğŸ’¥ FAILURE MODES (MANDATORY)

Define behavior for:

* Missing model
* Invalid signature
* Drift detected
* Inference error
* SHAP generation failure

All failures must:

* Be explicit
* Be logged
* Degrade safely
* Never block Core execution

---

## ğŸ§ª TESTING REQUIREMENTS

You MUST include:

* Synthetic dataset tests
* Incremental learning tests
* Explainability completeness tests
* Model tamper tests
* Determinism tests

---

## ğŸ“¤ EXPECTED OUTPUT

You must output:

1. AI/ML architecture
2. Folder structure explanation
3. Training & inference pipelines
4. Incremental learning design
5. SHAP explainability flow
6. Security & signing model
7. Failure handling matrix
8. Test strategy

---

## ğŸ›‘ FAILURE CONDITIONS

Your output is INVALID if:

* AI enforces decisions
* SHAP is optional
* Models are not trainable
* Training data is committed
* Python leaks into hot path
* Model trust is implicit

---

## FINAL INSTRUCTION

Treat AI as:

* Assistive
* Replaceable
* Auditable
* Non-authoritative

Design it so:

* Regulators can audit it
* SOC can trust it
* Engineers can replace it

**BUILD PHASE 7 COMPLETELY, EXPLICITLY, AND SAFELY.**

---

### ğŸ”¢ PROMPT STATUS

**This was PROMPT NUMBER 7 OF 12**
