# ğŸ§  RANSOMEYE â€“ PHASE 8

## LLM Summarization, RAG & SOC Copilot (Advisory Only)

### **(PROMPT NUMBER 8 OF 12)**

**MASTER PROMPT â€“ CURSOR EXECUTION**

---

## CONTEXT (STRICTLY ADVISORY ROLE)

You are building the **LLM-based Summarization and SOC Copilot layer** for RansomEye.

This component exists **only to assist humans**.
It must **never influence enforcement, policy evaluation, or automated response**.

Any violation here introduces:

* Legal risk
* Safety risk
* Trust-model collapse

This phase must be **hard-isolated** from the Core decision path.

---

## ğŸš¨ ABSOLUTE LANGUAGE RULES

### âœ… ALLOWED LANGUAGES

* **Python (MANDATORY for LLM orchestration)**
* **Rust (ONLY for adapters, verification, and isolation wrappers)**

### âŒ FORBIDDEN LANGUAGES

* Java
* Go
* NodeJS
* Any cloud-only SDK
* Any external APIâ€“dependent LLM

â¡ï¸ LLMs must run **locally / offline**.

---

## ğŸš« LICENSE RULES (STRICT)

Allowed:

* MIT
* BSD
* Apache 2.0

BANNED:

* GPL / AGPL / SSPL
* Proprietary cloud LLM SDKs
* Unknown-license models or tokenizers

Models must be:

* Locally hosted
* Redistributable
* License-reviewed

---

## ğŸ¯ PHASE 8 OBJECTIVE

Build a **read-only, offline, explainable SOC Copilot** that:

1. Summarizes incidents
2. Explains correlation and policy decisions
3. Answers analyst questions using RAG
4. Produces **human-readable narratives**
5. Never produces commands or actions

---

## ğŸ“¦ SOC COPILOT RESPONSIBILITIES (STRICT)

### MUST DO

* Contextual summarization
* Natural-language explanation
* Evidence referencing
* Analyst Q&A (read-only)
* Confidence and uncertainty disclosure

### MUST NEVER DO

* Trigger actions
* Modify state
* Suggest enforcement
* Bypass policy
* Learn autonomously

---

## ğŸ“ PHASE 8 FOLDER STRUCTURE (MANDATORY)

You MUST create and explain:

```
ai/
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ model_registry.json
â”‚   â”‚   â””â”€â”€ license_manifest.md
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ incident_summary.txt
â”‚   â”‚   â”œâ”€â”€ kill_chain_explanation.txt
â”‚   â”‚   â”œâ”€â”€ policy_explanation.txt
â”‚   â”‚   â””â”€â”€ analyst_qa.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ document_indexer.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â””â”€â”€ retrieval.py
â”‚   â”‚
â”‚   â”œâ”€â”€ execution/
â”‚   â”‚   â”œâ”€â”€ llm_runtime.py
â”‚   â”‚   â”œâ”€â”€ sandbox.py
â”‚   â”‚   â””â”€â”€ guardrails.py
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ read_only_adapter.py
â”‚   â”‚   â””â”€â”€ schema_validation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ security/
â”‚   â”‚   â”œâ”€â”€ prompt_signing.py
â”‚   â”‚   â”œâ”€â”€ model_verification.py
â”‚   â”‚   â””â”€â”€ output_sanitization.py
â”‚   â”‚
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ soc_copilot_scope.md
â”‚   â”‚   â”œâ”€â”€ rag_design.md
â”‚   â”‚   â”œâ”€â”€ prompt_governance.md
â”‚   â”‚   â””â”€â”€ failure_modes.md
â”‚   â”‚
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ hallucination_tests.py
â”‚       â”œâ”€â”€ isolation_tests.py
â”‚       â”œâ”€â”€ prompt_integrity_tests.py
â”‚       â””â”€â”€ regression_tests.py
```

---

## ğŸ§  LLM MODEL CONSTRAINTS

You MUST:

* Use local models only (e.g., llama.cppâ€“based)
* Verify model checksum and signature
* Pin model versions
* Document license for every model

Model load failure = **Copilot disabled**, not system failure.

---

## ğŸ“š RAG (RETRIEVAL-AUGMENTED GENERATION)

You MUST define:

* Allowed document sources:

  * Correlation explanations
  * Policy decisions
  * Audit logs
  * Runbooks
* Indexing strategy
* Retrieval limits
* Context window enforcement

No raw telemetry allowed.

---

## ğŸ” GUARDRAILS & SAFETY (MANDATORY)

You MUST enforce:

* Prompt signing
* Prompt immutability
* Output sanitization
* Hallucination detection
* Explicit uncertainty disclosure

LLM output must:

* Reference evidence IDs
* Indicate confidence
* Never fabricate facts

---

## ğŸ”€ CORE â†” LLM INTERACTION (STRICT)

* One-way, read-only
* LLM cannot call Core APIs that mutate state
* LLM cannot see secrets
* LLM cannot access raw events

---

## ğŸ’¥ FAILURE MODES (MANDATORY)

Define behavior for:

* Model missing
* Model signature invalid
* Prompt tampered
* RAG index corrupted
* Hallucination detected

Failures must:

* Be logged
* Be visible to SOC
* Never affect enforcement

---

## ğŸ§ª TESTING REQUIREMENTS

You MUST include:

* Hallucination tests
* Prompt integrity tests
* Read-only enforcement tests
* Regression tests
* Offline operation tests

---

## ğŸ“¤ EXPECTED OUTPUT

You must output:

1. SOC Copilot architecture
2. Folder structure explanation
3. Prompt governance model
4. RAG pipeline design
5. Guardrail enforcement
6. Failure handling matrix
7. Test strategy

---

## ğŸ›‘ FAILURE CONDITIONS

Your output is INVALID if:

* LLM can influence enforcement
* External APIs are used
* Prompts are mutable at runtime
* Model licenses are unclear
* Read-only boundary is violated

---

## FINAL INSTRUCTION

Treat the LLM as:

* A junior analyst assistant
* Not an authority
* Not a decision-maker

Design it so:

* Humans stay in control
* Evidence remains verifiable
* Outputs remain defensible

**BUILD PHASE 8 COMPLETELY, ISOLATED, AND SAFE.**

---

### ğŸ”¢ PROMPT STATUS

**This was PROMPT NUMBER 8 OF 12**

