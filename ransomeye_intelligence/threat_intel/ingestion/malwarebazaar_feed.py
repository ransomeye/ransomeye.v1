# Path and File Name : /home/ransomeye/rebuild/ransomeye_intelligence/threat_intel/ingestion/malwarebazaar_feed.py
# Author: nXxBku0CKFAJCBN3X1g3bQk7OxYQylg8CMw1iGsq7gU
# Details of functionality of this file: MalwareBazaar threat intelligence feed collector for training data (Phase 6 - Secure, Key-Safe)

"""
MalwareBazaar Feed Collector: Collects threat intelligence from MalwareBazaar API.
All data is cached locally for offline training use.
Phase 6: Secure, key-safe implementation with fail-safe logic.
"""

import os
import sys
import json
import subprocess
import hashlib
import socket
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import time

# MalwareBazaar API endpoint
MALWARBAZAAR_API_URL = "https://mb-api.abuse.ch/api/v1/"

# Environment variable for API key (MANDATORY when online)
ENV_API_KEY = "RANSOMEYE_FEED_MALWAREBAZAAR_API_KEY"

FEEDS_DIR = Path("/home/ransomeye/rebuild/ransomeye_intelligence/threat_intel/feeds")
CACHE_DIR = Path("/home/ransomeye/rebuild/ransomeye_intelligence/threat_intel/cache/malwarebazaar")


class FeedError(Exception):
    """Feed-specific error that does not crash the system."""
    pass


def check_internet_connectivity(timeout: int = 5) -> bool:
    """
    Check if internet connectivity is available.
    
    Args:
        timeout: Connection timeout in seconds
    
    Returns:
        True if internet is available, False otherwise
    """
    test_hosts = [
        ('8.8.8.8', 53),  # Google DNS
        ('1.1.1.1', 53),  # Cloudflare DNS
        ('mb-api.abuse.ch', 443),  # MalwareBazaar API
    ]
    
    for host, port in test_hosts:
        try:
            sock = socket.create_connection((host, port), timeout=timeout)
            sock.close()
            return True
        except (socket.error, OSError):
            continue
    
    return False


class MalwareBazaarFeedCollector:
    """Collects threat intelligence from MalwareBazaar API (Phase 6 - Secure, Key-Safe)."""
    
    def __init__(self, auth_key: Optional[str] = None):
        """
        Initialize MalwareBazaar feed collector.
        
        Args:
            auth_key: Optional API key (overrides environment variable)
        
        Raises:
            FeedError: If internet is available but API key is missing
        """
        # Read API key from environment (MANDATORY when online)
        self.auth_key = auth_key or os.getenv(ENV_API_KEY)
        self.api_url = MALWARBAZAAR_API_URL
        FEEDS_DIR.mkdir(parents=True, exist_ok=True)
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
        
        # Phase 6: Fail-safe logic - if internet available and key missing, fail feed (not system)
        if check_internet_connectivity():
            if not self.auth_key:
                raise FeedError(
                    f"Internet is available but {ENV_API_KEY} is not set. "
                    "Feed will fail, but system continues running."
                )
    
    def _fetch_sample_info(self, hash_value: str) -> Optional[Dict]:
        """
        Fetch sample information from MalwareBazaar API.
        
        Args:
            hash_value: SHA256 hash of the sample
        
        Returns:
            Sample information or None if not found
        """
        try:
            # Use wget to fetch data (offline-capable, can be cached)
            cmd = [
                'wget',
                '--header', f'Auth-Key: {self.auth_key}',
                '--post-data', f'query=get_info&hash={hash_value}',
                self.api_url,
                '-O-',
                '--timeout=30',
                '--tries=1'
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode != 0:
                print(f"Warning: Failed to fetch hash {hash_value}: {result.stderr}", file=sys.stderr)
                return None
            
            try:
                data = json.loads(result.stdout)
                if data.get('query_status') == 'ok':
                    return data.get('data', [{}])[0] if data.get('data') else None
                else:
                    print(f"Warning: Query failed for {hash_value}: {data.get('query_status')}", file=sys.stderr)
                    return None
            except json.JSONDecodeError:
                print(f"Warning: Invalid JSON response for {hash_value}", file=sys.stderr)
                return None
        
        except subprocess.TimeoutExpired:
            print(f"Warning: Timeout fetching {hash_value}", file=sys.stderr)
            return None
        except Exception as e:
            print(f"Warning: Error fetching {hash_value}: {e}", file=sys.stderr)
            return None
    
    def fetch_recent_samples(self, limit: int = 100) -> Tuple[List[Dict], bool]:
        """
        Fetch recent malware samples from MalwareBazaar (metadata-only).
        
        Args:
            limit: Maximum number of samples to fetch
        
        Returns:
            Tuple of (samples list, success flag)
        
        Raises:
            FeedError: If feed fetch fails (system continues)
        """
        # Phase 6: Check internet and key before attempting fetch
        if check_internet_connectivity():
            if not self.auth_key:
                raise FeedError(
                    f"Internet available but {ENV_API_KEY} missing. Feed fails, system continues."
                )
        else:
            # Offline mode - return empty list, not an error
            print("Info: Offline mode - returning cached samples only", file=sys.stderr)
            return [], False
        
        samples = []
        
        # Fetch recent samples (query_type=recent) - metadata only, no binary downloads
        try:
            cmd = [
                'wget',
                '--header', f'Auth-Key: {self.auth_key}',
                '--post-data', f'query=get_recent&selector=100',
                self.api_url,
                '-O-',
                '--timeout=60',
                '--tries=1'
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode == 0:
                try:
                    data = json.loads(result.stdout)
                    if data.get('query_status') == 'ok':
                        sample_list = data.get('data', [])
                        for sample in sample_list[:limit]:
                            # Extract metadata only (hash, family, tags) - no binary downloads
                            metadata = {
                                'sha256_hash': sample.get('sha256_hash', ''),
                                'md5_hash': sample.get('md5_hash', ''),
                                'sha1_hash': sample.get('sha1_hash', ''),
                                'file_name': sample.get('file_name', ''),
                                'file_type': sample.get('file_type', ''),
                                'file_size': sample.get('file_size', 0),
                                'first_seen': sample.get('first_seen', ''),
                                'last_seen': sample.get('last_seen', ''),
                                'signature': sample.get('signature', ''),
                                'tags': sample.get('tags', []),
                                'reporter': sample.get('reporter', ''),
                                'yara_rules': sample.get('yara_rules', []),
                            }
                            # Get detailed info for each sample (metadata only)
                            sha256 = sample.get('sha256_hash', '')
                            if sha256:
                                detailed = self._fetch_sample_info(sha256)
                                if detailed:
                                    # Merge metadata
                                    metadata.update({
                                        'family': detailed.get('family', ''),
                                        'imphash': detailed.get('imphash', ''),
                                        'ssdeep': detailed.get('ssdeep', ''),
                                        'tlsh': detailed.get('tlsh', ''),
                                    })
                                    samples.append(metadata)
                                time.sleep(0.5)  # Rate limiting
                    else:
                        raise FeedError(f"MalwareBazaar API query failed: {data.get('query_status')}")
                except json.JSONDecodeError:
                    raise FeedError("Invalid JSON response from MalwareBazaar API")
            else:
                raise FeedError(f"wget failed: {result.stderr}")
        except subprocess.TimeoutExpired:
            raise FeedError("Timeout fetching from MalwareBazaar API")
        except FeedError:
            raise
        except Exception as e:
            raise FeedError(f"Error fetching recent samples: {e}")
        
        return samples, True
    
    def cache_samples(self, samples: List[Dict], feed_id: str = None) -> Path:
        """
        Cache samples to local file for offline training.
        Verifies integrity and normalizes to feature vectors.
        
        Args:
            samples: List of sample data
            feed_id: Optional feed identifier
        
        Returns:
            Path to cached file
        """
        if feed_id is None:
            feed_id = f"malwarebazaar_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        cache_path = CACHE_DIR / f"{feed_id}.json"
        
        feed_data = {
            'feed_id': feed_id,
            'source': 'malwarebazaar',
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'sample_count': len(samples),
            'samples': samples,
            'feed_hash': None
        }
        
        # Phase 6: Compute feed hash for integrity verification
        feed_json = json.dumps(feed_data, sort_keys=True)
        feed_hash = hashlib.sha256(feed_json.encode()).hexdigest()
        feed_data['feed_hash'] = f"sha256:{feed_hash}"
        
        # Phase 6: Normalize to feature vectors
        normalized_features = self._normalize_to_features(samples)
        feed_data['normalized_features'] = normalized_features
        
        # Save to cache
        with open(cache_path, 'w') as f:
            json.dump(feed_data, f, indent=2)
        
        print(f"  âœ“ Cached {len(samples)} samples to {cache_path}")
        return cache_path
    
    def _normalize_to_features(self, samples: List[Dict]) -> List[Dict]:
        """
        Normalize samples to feature vectors for training.
        
        Args:
            samples: List of sample metadata
        
        Returns:
            List of normalized feature vectors
        """
        features = []
        for sample in samples:
            feature_vector = {
                'hash': sample.get('sha256_hash', ''),
                'family': sample.get('family', ''),
                'tags': sample.get('tags', []),
                'file_type': sample.get('file_type', ''),
                'file_size': sample.get('file_size', 0),
                'signature': sample.get('signature', ''),
                'yara_rules_count': len(sample.get('yara_rules', [])),
                'has_imphash': bool(sample.get('imphash')),
                'has_ssdeep': bool(sample.get('ssdeep')),
                'has_tlsh': bool(sample.get('tlsh')),
            }
            features.append(feature_vector)
        return features
    
    def load_cached_samples(self) -> List[Dict]:
        """Load all cached samples."""
        all_samples = []
        
        for cache_file in sorted(CACHE_DIR.glob("*.json")):
            try:
                with open(cache_file, 'r') as f:
                    feed_data = json.load(f)
                    samples = feed_data.get('samples', [])
                    all_samples.extend(samples)
            except Exception:
                continue
        
        return all_samples


def main():
    """CLI entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(description='MalwareBazaar Feed Collector (Phase 6 - Secure)')
    parser.add_argument('--limit', type=int, default=100,
                       help='Maximum number of samples to fetch')
    parser.add_argument('--cache-only', action='store_true',
                       help='Only load from cache, do not fetch new data')
    parser.add_argument('--auth-key', default=None,
                       help=f'MalwareBazaar API auth key (or use {ENV_API_KEY} env var)')
    
    args = parser.parse_args()
    
    try:
        collector = MalwareBazaarFeedCollector(auth_key=args.auth_key)
    except FeedError as e:
        print(f"ERROR: {e}", file=sys.stderr)
        print("Feed fails, but system continues running.", file=sys.stderr)
        sys.exit(1)
    
    if args.cache_only:
        samples = collector.load_cached_samples()
        print(f"Loaded {len(samples)} samples from cache")
    else:
        print("Fetching recent samples from MalwareBazaar...")
        try:
            samples, success = collector.fetch_recent_samples(limit=args.limit)
            if success:
                print(f"Fetched {len(samples)} samples")
                if samples:
                    cache_path = collector.cache_samples(samples)
                    print(f"Cached to: {cache_path}")
            else:
                print("Offline mode - no new samples fetched", file=sys.stderr)
        except FeedError as e:
            print(f"ERROR: {e}", file=sys.stderr)
            print("Feed fails, but system continues running.", file=sys.stderr)
            sys.exit(1)
    
    return samples


if __name__ == '__main__':
    main()

