# Path and File Name : /home/ransomeye/rebuild/ransomeye_intelligence/threat_intel/ingestion/malwarebazaar_feed.py
# Author: nXxBku0CKFAJCBN3X1g3bQk7OxYQylg8CMw1iGsq7gU
# Details of functionality of this file: MalwareBazaar threat intelligence feed collector for training data

"""
MalwareBazaar Feed Collector: Collects threat intelligence from MalwareBazaar API.
All data is cached locally for offline training use.
"""

import os
import sys
import json
import subprocess
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import time

# MalwareBazaar API credentials
MALWARBAZAAR_API_URL = "https://mb-api.abuse.ch/api/v1/"
MALWARBAZAAR_AUTH_KEY = os.getenv("MALWARBAZAAR_AUTH_KEY", "483ce60ba7c8a3d7358e3c8afd6e6d23a746eb2a5a42479f")

FEEDS_DIR = Path("/home/ransomeye/rebuild/ransomeye_intelligence/threat_intel/feeds")
CACHE_DIR = Path("/home/ransomeye/rebuild/ransomeye_intelligence/threat_intel/cache/malwarebazaar")


class MalwareBazaarFeedCollector:
    """Collects threat intelligence from MalwareBazaar API."""
    
    def __init__(self, auth_key: Optional[str] = None):
        self.auth_key = auth_key or MALWARBAZAAR_AUTH_KEY
        self.api_url = MALWARBAZAAR_API_URL
        FEEDS_DIR.mkdir(parents=True, exist_ok=True)
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    def _fetch_sample_info(self, hash_value: str) -> Optional[Dict]:
        """
        Fetch sample information from MalwareBazaar API.
        
        Args:
            hash_value: SHA256 hash of the sample
        
        Returns:
            Sample information or None if not found
        """
        try:
            # Use wget to fetch data (offline-capable, can be cached)
            cmd = [
                'wget',
                '--header', f'Auth-Key: {self.auth_key}',
                '--post-data', f'query=get_info&hash={hash_value}',
                self.api_url,
                '-O-',
                '--timeout=30',
                '--tries=1'
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode != 0:
                print(f"Warning: Failed to fetch hash {hash_value}: {result.stderr}", file=sys.stderr)
                return None
            
            try:
                data = json.loads(result.stdout)
                if data.get('query_status') == 'ok':
                    return data.get('data', [{}])[0] if data.get('data') else None
                else:
                    print(f"Warning: Query failed for {hash_value}: {data.get('query_status')}", file=sys.stderr)
                    return None
            except json.JSONDecodeError:
                print(f"Warning: Invalid JSON response for {hash_value}", file=sys.stderr)
                return None
        
        except subprocess.TimeoutExpired:
            print(f"Warning: Timeout fetching {hash_value}", file=sys.stderr)
            return None
        except Exception as e:
            print(f"Warning: Error fetching {hash_value}: {e}", file=sys.stderr)
            return None
    
    def fetch_recent_samples(self, limit: int = 100) -> List[Dict]:
        """
        Fetch recent malware samples from MalwareBazaar.
        
        Args:
            limit: Maximum number of samples to fetch
        
        Returns:
            List of sample information
        """
        samples = []
        
        # Fetch recent samples (query_type=recent)
        try:
            cmd = [
                'wget',
                '--header', f'Auth-Key: {self.auth_key}',
                '--post-data', f'query=get_recent&selector=100',
                self.api_url,
                '-O-',
                '--timeout=60',
                '--tries=1'
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode == 0:
                try:
                    data = json.loads(result.stdout)
                    if data.get('query_status') == 'ok':
                        sample_list = data.get('data', [])
                        for sample in sample_list[:limit]:
                            # Get detailed info for each sample
                            sha256 = sample.get('sha256_hash', '')
                            if sha256:
                                detailed = self._fetch_sample_info(sha256)
                                if detailed:
                                    samples.append(detailed)
                                time.sleep(0.5)  # Rate limiting
                except json.JSONDecodeError:
                    print("Warning: Invalid JSON response for recent samples", file=sys.stderr)
        except Exception as e:
            print(f"Warning: Error fetching recent samples: {e}", file=sys.stderr)
        
        return samples
    
    def cache_samples(self, samples: List[Dict], feed_id: str = None) -> Path:
        """
        Cache samples to local file for offline training.
        
        Args:
            samples: List of sample data
            feed_id: Optional feed identifier
        
        Returns:
            Path to cached file
        """
        if feed_id is None:
            feed_id = f"malwarebazaar_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        cache_path = CACHE_DIR / f"{feed_id}.json"
        
        feed_data = {
            'feed_id': feed_id,
            'source': 'malwarebazaar',
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'sample_count': len(samples),
            'samples': samples,
            'feed_hash': None
        }
        
        # Compute feed hash
        feed_json = json.dumps(feed_data, sort_keys=True)
        feed_hash = hashlib.sha256(feed_json.encode()).hexdigest()
        feed_data['feed_hash'] = f"sha256:{feed_hash}"
        
        # Save to cache
        with open(cache_path, 'w') as f:
            json.dump(feed_data, f, indent=2)
        
        print(f"  âœ“ Cached {len(samples)} samples to {cache_path}")
        return cache_path
    
    def load_cached_samples(self) -> List[Dict]:
        """Load all cached samples."""
        all_samples = []
        
        for cache_file in sorted(CACHE_DIR.glob("*.json")):
            try:
                with open(cache_file, 'r') as f:
                    feed_data = json.load(f)
                    samples = feed_data.get('samples', [])
                    all_samples.extend(samples)
            except Exception:
                continue
        
        return all_samples


def main():
    """CLI entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(description='MalwareBazaar Feed Collector')
    parser.add_argument('--limit', type=int, default=100,
                       help='Maximum number of samples to fetch')
    parser.add_argument('--cache-only', action='store_true',
                       help='Only load from cache, do not fetch new data')
    parser.add_argument('--auth-key', default=None,
                       help='MalwareBazaar API auth key (or use MALWARBAZAAR_AUTH_KEY env var)')
    
    args = parser.parse_args()
    
    collector = MalwareBazaarFeedCollector(auth_key=args.auth_key)
    
    if args.cache_only:
        samples = collector.load_cached_samples()
        print(f"Loaded {len(samples)} samples from cache")
    else:
        print("Fetching recent samples from MalwareBazaar...")
        samples = collector.fetch_recent_samples(limit=args.limit)
        print(f"Fetched {len(samples)} samples")
        
        if samples:
            cache_path = collector.cache_samples(samples)
            print(f"Cached to: {cache_path}")
    
    return samples


if __name__ == '__main__':
    main()

